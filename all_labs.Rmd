---
title: "Computer Labs"
# author: 'SID: 510472357'
date: "University of Sydney | Semester 2 `r format(Sys.time(), '%Y')`"
output: 
  html_document:
    code_folding: show  
    code_download: true 
    toc: true 
    toc_depth: 3
    toc_float: true 
    number_section: false 
    theme:
      bootswatch: "cosmo"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

* `pbinom`: cdf

* `dbinom`: pdf

* `qbinom`: quantile 

* `rbinom`: generate random values


# Week 2

## Question 1

**a)** Using `rbinom`, generate 1000 Binomial(200, 0.3) random variables and store them in `X`. Plot a histogram of `X` using `hist`, and overlay it with a normal density curve with appropriate mean and standard deviation using `curve` and `dnorm`.


```{r}
set.seed(4023)

X<-rbinom(1000,200,0.3)
hist(X, prob=TRUE, main="distribution of binomial(200,0.3)")
x_mean<-200*0.3
x_sd<-sqrt(200*0.3*0.7)
curve(dnorm(x,mean=x_mean, sd=x_sd), xlim=c(0,200), add=T)
```

**b)** Find $P(45 ≤ X ≤ 55)$ using either pbinom or dbinom.

```{r}
pbinom(55,200,0.3)-pbinom(44,200,0.3)
```


```{r}
dbinom(45:55,200,0.3)
sum(dbinom(45:55,200,0.3))
```

**c)** Using `pnorm`, compute the same probability with the normal approximation and normal
approximation with continuity correction. Comment on how close the approximations are.

Normal approximation:

```{r}
pnorm(55, mean=x_mean, sd=x_sd) - pnorm(45,mean=x_mean, sd=x_sd) 
```

Normal approximation with continuity correction gives a closer approximation.

```{r}
pnorm(55.5, mean=x_mean, sd=x_sd) - pnorm(44.5, mean=x_mean, sd=x_sd)
```

- the normal cdf cannot approximate binom cdf well at the jump points, bc its continuous whereas binom is discrete, but it approximates it very well half-way between these points of the binom cdf

## Question 2

**a)** Generate 1000 Binomial(200, 0.03) random variables (`rbinom`) and 1000 Poisson(6) random variables (`rpois`), store them in `X` and `Y` respectively. Set up a plotting environment with 1 row and 2 columns (`par(mfrow=c(1,2))`), and generate histograms of `X` and `Y`. Comment on the similarity of the two distributions.

```{r}
set.seed(3023)
par(mfrow=c(1,2))
X<-rbinom(1000,200,0.03)
hist(X, breaks=15, prob=TRUE, xlim=c(0,15), ylim=c(0,0.18), main="Binomial(200,0.03)")
Y<-rpois(1000,6)
hist(Y, breaks=15, prob=TRUE, xlim=c(0,15), ylim=c(0,0.18), main="Poisson(6)")
```

- as long as the mean of the binom is not too close to the end points (as long as theres no major skewness) normal is good approximator
- otherwise use poisson - better

**b)** Find $P (X ≤ 5)$ with `pbinom` and $P (Y ≤ 5)$ with `ppois`. Compare the two results.

```{r}
pbinom(5,200,0.03)
ppois(5,6)
```

- very close


**c)** Approximate $P (X ≤ 5)$ with the normal approximation (`pnorm`). How close is it this time?

```{r}
pnorm(5,mean=200*0.03,sd=sqrt(200*0.03*0.97))
```

- not accurate in this case 

## Question 3

Recall that for $n$ iid random variables $X_1,...,X_n,$ the standardised sum is given by

$$S = \frac{\sum_{i=1}^{n} (X_i - E(X_i))}{\sqrt{n\text{Var}(X_1)}}.$$

**a)** Generate 1000 realisations of the sum of $n$ $(n = 5)$ Unif(0,1) random variables (using `runif`), compute the standardised the sums and store them in `S1`. Repeat for $n = 100$ and store the results in `S2`.

Note that $E(X_i) = \frac{1}{2}$ and $\text{Var}(X_i) = \frac{1}{12}$

```{r}
set.seed(3023)

n <- 5
k <- 1000

result1 <- rep(0,k)

for (i in 1:k) {
  result1[i] <- sum(runif(n))
}

S1 <- (result1 - 0.5 * n) / sqrt(n * 1 / 12)



# repeating for n=100
n <- 100

result2 <- rep(0, k)
for (i in 1:k) {
  result2[i] <- sum(runif(n))
}

S2 <- (result2 - 0.5 * n) / sqrt(n * 1 / 12
)
```

**b)** Plot histograms of `S1` and `S2`, overlay them with the density curve of standard normal, and place these two plots side by side using `par(mfrow=c(1,2))`. Comment on how close the normal approximation is for the two values of $n$. 

```{r}
par(mfrow=c(1,2))
hist(S1, prob=TRUE, ylim=c(0,0.4), xlim=c(-3,3))
curve(dnorm(x,mean=0, sd=1),add=T)
hist(S2, prob=TRUE, ylim=c(0,0.4), xlim=c(-3,3))
curve(dnorm(x,mean=0, sd=1),add=T)
```

- both n give good approximations

**c)** Repeat (a) and (b) for the sum of lognormal(0,1) distributions (use `rlnorm(n, meanlog = 0, sdlog = 1)`). In this case, $E(X_i) = exp(1/2), Var(X_i) = (exp(1) − 1) exp(1).$

```{r}
set.seed(3023)


n <- 5
k <- 1000
result1 <- rep(0, k)

for (i in 1:k) {
  result1[i] <- sum(rlnorm(n, meanlog = 0, sdlog = 1))
}

S1 = (result1 - exp(1 / 2) * n) / sqrt(n * (exp(1) - 1) * exp(1))


n <- 100
k <- 1000
result2 <- rep(0, k)

for (i in 1:k) {
  result2[i] <- sum(rlnorm(n, meanlog = 0, sdlog = 1))
}

S2 <- (result2 - exp(1 / 2) * n) / sqrt(n * (exp(1) - 1) * exp(1))


par(mfrow = c(1, 2))
hist(S1, prob = TRUE)
curve(dnorm(x, mean = 0, sd = 1), add = T)
hist(S2, prob = TRUE)
curve(dnorm(x, mean = 0, sd = 1), add = T)
```


- In this case, the approximation gets better for larger $n$. Note that lognormal has a heavy tail distribution, so the convergence is slower.


**Q:** how many samples do we need before we can use normal dist to approximate a dist?

Ans: it depends on how "non-normal" the dist is. if perfectly normal, then even very small sample size is sufficient to use normal dist



# Week 3

## Question 1

**a)** Generate 100 realisations of the sample variance of 8 independent $N(0,1)$ random variables and store them in `s2`.

```{r}
set.seed(4023)

s2 <- vector()

for (i in 1:100) {
  s2[i] <- # storing the result as the ith element of s2
    var(rnorm(8, mean = 0, sd = 1)) # generating 8 iid N(0,1) RVs and calculating their variance
}
```

**b)** Plot the histogram of `(8-1)*s2` and overlay it with the density function of the $\chi^2_7$ distribution (use `dchisq`).

```{r}
hist((8-1)*s2, prob=TRUE)
curve(dchisq(x, df = 7), xlim=c(0,50), add=T)
```


**c)** Repeat(a) and (b) with $n = 50$ independent $N(0,1)$ random variables. Overlay the histogram with both the density curve of $\chi^2_{n−1}$ for an appropriate $\nu$ and the density curve of $N (n − 1, 2(n − 1))$ (using two different colours). Comment on the fit. Explain *briefly* why the normal approximation is suitable here.

```{r}
set.seed(4023)

s2 <- vector() # initialising vector 

for (i in 1:100) {
  s2[i] <- # storing the result as the ith element of s2
    var(rnorm(50, mean = 0, sd = 1)) # generating 50 iid N(0,1) RVs and calculating their variance
}

hist((49)*s2, prob=TRUE)
curve(dchisq(x, df = 49), xlim=c(0,100), add=T, col = "red", lty=1)
curve(dnorm(x, mean = 49, sd = (2*49)**0.5), add = T, col = "blue", lty=1)
legend("topright", legend = c(expression(chi[n-1]^2), expression(N(n-1,2(n-1)))),
       col = c("red", "blue"), lty=1:1, cex=0.8)
```

The density curve of $N (n − 1, 2(n − 1))$ closely follows the density curve of $\chi^2_{n−1}$, and fits the histogram as well as the density curve of $\chi^2_{n-1}$, hence using normal approximation is suitable here. Note that the $\chi^2$ density curve more closely follows the histogram here than in the previous question, since here we have a greater $n$. 

- when the df is large, we can apply the normal approximation due to the CLT

**d)** For $n = 50$, compute $P ((n − 1) \cdot S^2 > 54)$ using both the exact distribution ($\chi^2_\nu$) and the normal approximation. Compare the results.

```{r}
n = 50

# using exact distribution:
1 - pchisq(54, df = n-1) 

# using normal approximation:
1 - pnorm(54, mean = n-1, 
          sd = (2*(n-1))**0.5) # sqrt
```


Using the exact distribution and normal approximation yields very similar probabilities. The normal approximation gives a slightly higher value as its peak is slightly more to the right. 

- reasonably close


## Question 2

**a)** When two random variables $(X, Y )$ follow a bivariate normal distribution, the covariance matrix $\Sigma$ is defined as 

$$\Sigma = \begin{pmatrix}
\sigma_1^2 & \rho \sigma_1 \sigma_2 \\
\rho \sigma_1 \sigma_2 & \sigma_2^2
\end{pmatrix}, $$

where $\rho$ is the correlation, $\sigma_1^2, \sigma_2^2$ are the variances of $X$ and $Y$ respectively. Use `mvrnorm` from the MASS library (use `library(MASS)`) to generate 500 samples from a bivariate normal distribution with $\mu = (\mu_1,\mu_2)$ with $\mu_1 = 1, \mu_2 = 2,$ and 

$$\Sigma = \begin{pmatrix}
1 & 0.5 \\
0.5 & 4
\end{pmatrix}. $$

Store the first column as `x` and the second column as `y`.


```{r}
set.seed(4023)
library(MASS)

sigma <- matrix(c(1,0.5,0.5,4),2,2) # specifying the covariance matrix

q2a <- mvrnorm(n = 500, mu = c(1,2), Sigma = sigma)
x <- q2a[,1]
y <- q2a[,2]
```


**b)** Plot the histogram of `x` and overlay it with the corresponding marginal normal density. Repeat for `y`. (Recall the marginal distribution of $X$ is $N(\mu_1,\sigma_1^2)$.)

```{r}
hist(x, prob=TRUE)
curve(dnorm(x, mean = 1, sd = 1), xlim=c(-10,10), add=T)
```

```{r}
hist(y, prob=TRUE)
curve(dnorm(x, mean = 2, sd = 2), xlim=c(-10,10), add=T)
```


**c)** Produce a scatter plot of `x` and `y` (use `plot`). Compute the sample correlation coefficient (use `cor`) and comment on how close it is to the population correlation $\rho$. (First work out $\rho$ in the $\Sigma$ given.) 

```{r}
plot(x,y,
     xlab = "x", ylab = "y")

cor(x,y)
```

Theoretically, $\sigma_1 = 1$ and $\sigma_2 = 2 \Rightarrow$ population correlation coefficient $\rho = 0.25$, which is fairly close to the sample correlation coefficient, 0.296. 

## Question 3

**a)** Generate 100 realisations of the uniform U(0,1) distribution and store it in `u`. Apply the transformation $z = − log (u)$. Verify that $z$ is a sample from an exponential(1) distribution by plot the histogram of $z$ and overlay it with the density of the distribution.

```{r}
set.seed(3023)

u <- runif(n = 100)
z <- -1*log(u)

hist(z, freq = FALSE)
curve(dexp(x, rate = 1), add=T)
```

**b)** Generate 100 realisations of the minimum of 10 independent exponential(1) random variables. Note the rate parameter in `rexp` is defined as the reciprocal of the expectation (check the density function in the help file `?rexp`).

```{r}
set.seed(3023)

x_mat <- 
  rexp(n=100*10, rate = 1) |> matrix(nrow=10, ncol=100)

# we need to take the minimum of each of 10 trials
y_mat <- apply(x_mat, 
               2, # applying over columns (1 if row)
               min)
```

alternative:

```{r}
# x_mat <- rexp(n=100*10, rate = 1) |> matrix(ncol = 10, nrow = 100)
# y_mat <- apply(x_mat,
#                1,
#                min)
```


**c)** Plot the histogram and overlay it with the density of exponential(1/10) (`rate=10`) distribution. Comment on the fit.

```{r}
hist(y_mat, freq = FALSE, main = "minimum of iid exponential")
curve(dexp(x, rate=10), add = TRUE)
```


# Week 4

## Question 1

**a)** Generate 500 iid Poisson(5) (use `rpois`) random variables and store them in a vector `N`.

```{r}
N <- rpois(n=500, lambda = 5)
```


**b)** Now for each generated element `n` in `N`, generate 1 random variable following a binomial distribution with $n$ trials and success probabilities 0.20. Store all the generated values into a vector `X`.

```{r}
X <- rbinom(n=500,
       size = N, # size of trials (n)
       prob = 0.2)
```

**c)** Plot the histogram of `X`. What is the marginal distribution of $X$? Compare the relatively frequency for each value in $X$ versus the pmf of this distribution.

```{r}
hist(X)
```

- X has a Poisson(5*0.2) = Poisson(1) distribution 

Frequency of each value in X:

```{r}
table(X)/500
```

pmf of Poisson(1):

```{r}
dpois(min(X):max(X), lambda = 1) |> round(3)
```

- relative frequency of each outcome is very close to the pmf evaluated by the poisson distribution with rate $\lambda = 1$

## Question 2: Transformation of Random Variables

**a)** Generate 100 random variables from the standard normal distribution, and store them in a vector `z`. Apply the transformation `u <- z^2`. Overlay the histogram of `u` with the density curve of a $\chi^2_1$ distribution (use `dchisq(x, 1)`. Also verify that the $\chi^2_1$ distribution is the same as the Gamma distribution with rate 1/2 and scale 2 (use `dgamma(x, shape = 1/2, scale = 2)`), Comment on the plot.


```{r}
z <- rnorm(n=100)
u <- z^2

hist(u, freq = FALSE)
curve(dchisq(x, 1), add=TRUE, col="blue")
curve(dgamma(x, shape = 1/2, scale = 2), add=TRUE, col="red", lty=2)
```

- the 3 distributions agree

**b)** Generate 100 random variables from a $t$ distribution with 5 degrees of freedom (use `rt(100,df=5)`). Store them in $t$. Make another vector `f` by `f <- t^2`. Overlay the histogram of `f` with the density curve of a $F_{1,5}$ distribution (use `df(x, df1=1, df2=5)`). Comment on the plot. 

```{r}
t <- rt(100, df=5)
f <- t^2

hist(f, freq=FALSE, main="square of t distrib")
curve(df(x, df1=1, df2=5), add = TRUE)
```

- the 2 distributions agree

**c)** Generate 100 random variables from a $F_{5,2}$ distribution (use `rf(100, df1=5, df2=2)`). Store them in `y`. Make another vector `w <- 1/y`. Overlay the histogram of `w` with the density curve of a $F_{2,5}$ distribution. Comment on the plot.

```{r}
set.seed(3023)

y <- rf(100, df1=5, df2=2)
w <- 1/y

hist(w, freq=F, main="inverse of an F distrib", ylim=c(0,0.8))
curve(df(x, df1=2, df2=5), add = TRUE)
```

- the 2 distributions agree

**d)** Generate 100 random variables from a beta(2, 1) distribution (use `rbeta(100, shape1=2, shape2=1)`). Store them in `z`. Make another vector `v <- 2*z/(4*(1-z))`. Overlay the histogram of `v` with the density curve of a $F_{4,2}$ distribution. Comment on the plot.

```{r}
z <- rbeta(100, shape1 = 2, shape2 = 1)
v <- 2*z/(4*(1-z))

hist(v, freq=FALSE, ylim=c(0,0.5), main="transformation of beta distribution")
curve(df(x, df1=4, df2=2), add=TRUE)
```

- the 2 distributions agree


# Week 5

We shall compare three different estimators of a binomial success probability. If $Y ∼ Bin(2, θ)$ then we have

$$P(Y =0)=(1−θ)^2;$$

$$P(Y =1)=2θ(1−θ);$$

$$P(Y =2)=θ^2.$$

If we have an iid sample $Y_1,...,Y_n$ and define $N_0 = \sum_{i=1}^n 1 \{Yi = 0\}$ as the number of 0’s and $N_2 = \sum_{i=1}^n 1 \{Yi = 2\}$ as the number of 2’s, then we also have that

$$N_0 ∼ Bin(n,(1−θ)^2) \text{ and}$$

$$N_2 ∼ Bin(n,θ^2).$$


The usual estimator of θ based on an iid sample $Y_1,...,Y_n$ is a function of $\bar{Y} = \frac{1}{n} \sum_{i=1}^n Y_i$.

## Question 1

Determine an unbiased estimator for θ which is a *linear* function of $\bar{Y}$ Call it $\hat{\theta}_1$.

$$Y \sim Bin(2,\theta) \Rightarrow E(Y) = 2\theta$$

$$E(\bar{Y}) = \frac{1}{n} \sum_{i=1}^n E(Y_i) = \frac{1}{n} * n(2\theta) = 2\theta$$
$\Rightarrow \hat{\theta}_1 = \bar{Y}/2$ is an unbiased estimator of $\theta$

## Question 2

Determine an estimator for θ which is a *nonlinear* function of $N_0$ (hint: use method of moments, i.e. set $N_0$ equal to expectation and solve for θ). Call it $\hat{\theta}_0$. 

$E(N_0) = n(1-\theta)^2,$ setting $N_0 = n(1-\theta)^2$ gives $\hat{\theta}_0 = 1 - \sqrt{\frac{N_0}{n}}$

## Question 3

Determine an estimator for θ which is a *nonlinear* function of N2 (hint: use method of moments, i.e. set $N_2$ equal to expectation and solve for θ). Call it $\hat{\theta}_2$.

(similar)

$$\hat{\theta}_2 = \sqrt{\frac{N_2}{n}}$$

## Question 4

We shall simulate a sample of $n = 100$ iid $Y_i$’s and compute the values of these three estimators, followed by comparing their mean squared errors, for a fine grid of θ values. 

(full question on sheet)

```{r, eval=FALSE}
# NOT WORKING

th=(1:39)/40 # defining theta values

n = 100 # sample size
N = 10000 # number of iterations

# MSE0=MSE1=MSE2=0

MSE0=MSE1=MSE2= numeric(length(th))

L = length(th)

for (i in 1:L) {
  # print(i)
  th.hat1=th.hat0=th.hat2=0
  
  for (j in N) {
    Y <- rbinom(n, size = 2, p = th[i])
    th.hat1[j] <- mean(Y)/2
    
    N0 = sum(Y==0)
    N2 = sum(Y==2)
    
    th.hat0[j] <- 1 - sqrt(N0/n)
    th.hat2[j] <- sqrt(N2/n)
  }
  
  MSE0[i] <- mean((th.hat0-th[i])^2)
  MSE1[i] <- mean((th.hat1-th[i])^2)
  MSE2[i] <- mean((th.hat2-th[i])^2)
  
}
```

Plotting

```{r, eval=FALSE}
yrange=range(c(MSE0,MSE1,MSE2))

plot(th, MSE1, type='l', col='red', ylim=yrange, main = "Empirical MSEs")
lines(th, MSE0, col="blue")
```

(see solns for plot)

**e)** Comment on the plots and how they may or may not agree with theory, in particular explain the importance of the final curve added to the plot. If necessary, modify the legend to appropriately indicate this last curve. 

- the variance of any unbiased estimator is no smaller than the CRLB which is the inverse of the variance of the derivative of the log-likelihood wrt the parameter

- the CRLB is attained IFF the parameter is a linear function of the mean-value parameter (of the suff stat) in an exponential family, and this is attained by the corresponding linear function of the suff stat



# Week 6


## Question 4

**In R, generate a random sample of size $n = 50$ that follows a Poisson(1) distribution, using `rpois`. Compute the true value of the parameter $\theta$ and three estimates for $\theta$ on the generated sample.**

```{r}
set.seed(72357)

q4_sample <- rpois(n = 50, lambda = 1)

# true value of theta:
theta_true <- exp(-1)

# computing MLE estimator
x_bar <- mean(q4_sample)
theta_MLE <- exp(-1*x_bar)

# computing unbiased estimator
Y <- sum(q4_sample==0)
theta2 <- Y/50

# computing estimator theta_3, using question 2
S <- sum(q4_sample)
theta3 <- (1 - (1/50))^S
```

The true value of $\theta$ is `r round(theta_true,3)`, and the estimates based on the generated sample are:

- $\hat{\theta}_{MLE} =$ `r round(theta_MLE, 3)`
- $\hat{\theta}_{2} =$ `r round(theta2, 3)`
- $\hat{\theta}_{3} =$ `r round(theta3,3)`


## Question 5

**Perform a loop to see how the three estimators perform over a range of $\lambda$ values. Following the same basic method as in Week 5 Computer Lab, at each value of `lambda=seq(1, 5, length=40)`, generate `N=5000` Poisson samples of size `n=50` with that $\lambda$ value. Save the corresponding average squared errors for the three estimators.**

```{r}
set.seed(72357)

lambda_vals <- seq(1, 5, length = 40)
n <- 50  # sample size
N <- 5000  # number of simulation iterations

MSE_MLE <- numeric(length(lambda_vals))
MSE_unbiased <- numeric(length(lambda_vals))
MSE_theta3 <- numeric(length(lambda_vals))

# looping over each lambda value
for (i in 1:length(lambda_vals)) {
  lambda <- lambda_vals[i]
  # initialising vectors to store squared errors for each estimator
  sq_errors_MLE <- numeric(N)
  sq_errors_unbiased <- numeric(N)
  sq_errors_theta3 <- numeric(N)
  
  # inner loop for simulation iterations
  for (j in 1:N) {
    # generating random sample
    sample <- rpois(n, lambda)
    
    # computing true theta
    true_theta <- exp(-lambda)
    
    ## (similar to question 4)
    
    # MLE estimator
    X_bar <- mean(sample)
    theta_MLE <- exp(-X_bar)
    sq_errors_MLE[j] <- (theta_MLE - true_theta)^2
    
    # unbiased estimator
    Y <- sum(sample == 0)
    theta2 <- Y / n
    sq_errors_unbiased[j] <- (theta2 - true_theta)^2
    
    # estimator theta3
    S <- sum(sample)
    theta3 <- (1 - 1/n)^S
    sq_errors_theta3[j] <- (theta3 - true_theta)^2
  }
  
  # computing average squared errors for each estimator
  MSE_MLE[i] <- mean(sq_errors_MLE)
  MSE_unbiased[i] <- mean(sq_errors_unbiased)
  MSE_theta3[i] <- mean(sq_errors_theta3)
}
```



## Question 6

**Plot the average squared errors of the three estimators as a function of $\lambda$ on the same plot. Distinguishing these lines by using different colors or line types and adding appropriate legends. Out of the three estimators, which one performs the worst?**


```{r}
# Plot the MSEs
yrange <- range(c(MSE_MLE, MSE_unbiased, MSE_theta3))
plot(lambda_vals, MSE_MLE, type = 'l', col = 'orange', ylim = yrange, ylab = 'Average Squared Error', xlab = 'Lambda', main = "Empirical MSEs")
lines(lambda_vals, MSE_unbiased, col = 'blue')
lines(lambda_vals, MSE_theta3, col = 'darkgreen')
legend('topright', legend = c('MLE', 'Unbiased', 'Theta3'), col = c('orange', 'blue', 'darkgreen'), lty = 1) 
```

Out of the three estimators, the unbiased estimator, $\hat{\theta}_{2}$, performed the worst as it yielded the highest MSE values over the given range of $\lambda \in [1,5].$


## Question 7

**Excluding the estimator with the worst performance, plot the *ratio* of the average squared error and the CRLB for the remaining two estimators as a function of $\lambda$ on the same plot. Does either of these two estimators achieve the CRLB for all values of $\lambda$? Which one is a better estimator? Explain *briefly* why this is expected.**

The unbiased estimator ($\hat{\theta}_{2}$) has been excluded. 

```{r}
set.seed(72357)

# calculating the CRLB for each lambda value
CRLB <- (1/n) * lambda_vals * exp(-2 * lambda_vals)

# calculating the ratio of MSE to CRLB for the MLE and theta3 estimators
MSE_to_CRLB_MLE <- MSE_MLE / CRLB
MSE_to_CRLB_theta3 <- MSE_theta3 / CRLB

plot(lambda_vals, MSE_to_CRLB_MLE, type = 'l', col = 'orange', ylim = range(c(MSE_to_CRLB_MLE, MSE_to_CRLB_theta3)), 
     ylab = 'Ratio of MSE to CRLB', xlab = 'Lambda', main = 'MSE to CRLB Ratios')
lines(lambda_vals, MSE_to_CRLB_theta3, col = 'darkgreen')
legend('topright', legend = c('MLE', 'Theta3'), col = c('orange', 'darkgreen'), lty = 1)
```

- Neither of these two estimators ($\hat{\theta}_{MLE}$ and $\hat{\theta}_{3}$) achieves the CRLB for all values of lambda because the ratio is not equal to 1 for all values of $\lambda.$
    
    * Theoretically, neither $\hat{\theta}_{MLE}$ nor $\hat{\theta}_{3}$ are linear functions of the sufficient statistic $S$, meaning they do not attain the CRLB. 

- $\hat{\theta}_{3}$ is a better estimator as it has a ratio closer to 1 than the MLE, thus it is closer to the CRLB. 

- This is expected because since the Poisson distribution belongs to a full exponential family, any function of the sufficient statistic is the best unbiased estimator of its expected value. In our setting, $S = \sum^n_{i=1} X_i$ is a sufficient statistic for $\lambda,$ and $Y/n$ is an unbiased estimator for $\theta$. Hence, $\hat{\theta}_3 = (1/n)E(Y|S)$ is the best unbiased estimator for $E(\hat{\theta}_3) = (1/n)E(Y) = \theta$.


# Week 7

Consider the test of $H_0 : θ = θ_0$ against $H_1: \theta \neq \theta_0$ based on $X \sim f(\cdot ; \gamma_0, \theta)$ (f is gamma with known shape $\gamma_0$ but unknown scale parameter $\theta$)

## Question 1

**a)**

```{r}
# both are equiv bc gamma with shape 1 is exp
a = qgamma(0.025, shape=1)
a = qexp(0.025)
a

b = qexp(1-0.025)
b
```

**b)**

```{r}
th = (250:1500)/1000 

# a_vals = qgamma(0.025, shape=1/th)
# b_vals = qgamma(1-0.025, shape=1/th)

power <- pexp(a, 1/th) + 1 - pexp(b, 1/th)

# power_theta = a_vals + b_vals

plot(x=th, y=power, type="l")
abline(h=0.05, lty=2)
```

**c)**

> **i**

```{r}
fn <- function(c, alpha) {
  c*exp(-1*c) + (log(alpha - 1 + exp(-1*c)))*(alpha - 1 + exp(-1*c))
}
```

> **ii**

the use of `eps` helps us stay away from the upper bound, since the function is trying to evaluate log(0)

```{r}
eps = 1e-5
uniroot(fn, lower=0, upper = qexp(0.05)-eps, alpha=0.05) # solving eqn to return c
```

> **iii**

```{r}
expon.umpu = function(alpha) {
  eps = 1e-5
  c <- uniroot(fn, lower=0, upper = qexp(0.05)-eps, alpha=0.05)$root
  upper.tail = alpha - (1 - exp(-c))
  d = -log(upper.tail)
  list(c=c, d=d)
}

expon.umpu(0.05)
```


**d)**

```{r}
th=(250:1500)/1000
power=pexp(a,1/th) + 1-pexp(b,1/th)
plot(th,power,type="l",main="Testing for an exponential mean")
       abline(h=.05,lty=2)


umpu=expon.umpu(.05)
umpu.power=pexp(umpu$c,1/th) + 1-pexp(umpu$d,1/th)
lines(th,umpu.power,col="red")
legend("top",leg=c("Power of equal-tailed test","Power of UMPU test"),
        col=c("black","red"),lty=c(1,1))
```

- The power function of the UMPU test never goes below the 0.05 level, thus it is unbiased.


## Question 2

**a)**

```{r}
gamma.root = function(c, n, alpha) {
  lower = pgamma(c, shape = n, scale = 1) # cdf at c to give lower tail 
  d = qgamma(1 - (alpha - lower), shape = n, scale = 1) # finding d given c
  # LHS of the 2nd condition and returning its diff from alpha
  pgamma(c, shape = (n + 1), scale = 1) + 1 - pgamma(d, shape = (n + 1), scale = 1) - alpha
  }

eps = 1e-8
upper = qgamma(0.05, shape = 5, scale = 1) - eps

uniroot(gamma.root, lower = 0, upper = upper, n = 5, alpha = 0.05)
```

```{r}
gamma.umpu = function(alpha,n) {
    eps=1e-8
    upper=qgamma(alpha,shape=n,scale=1)-eps
    c=uniroot(gamma.root,lower=0,upper=upper,n=n,alpha=alpha)$root
    lower=pgamma(c,shape=n,scale=1)
    d=qgamma(1-(alpha-lower),shape=n,scale=1)
    list(c=c,d=d)
}
     
gamma.umpu(.05,5)
```


**b)**

```{r}
gu=gamma.umpu(.05,5) # critical values
g.power=pgamma(gu$c,shape=5,scale=th)+1-pgamma(gu$d,shape=5,scale=th) # computing power

plot(th,g.power,type="l",ylim=c(0,1))
abline(h=.05,lty=2)

gu$c
gu$d
```

- reject H_0 if X is outside [c,d]
- do not reject H_0 if X is within (c,d) 


# Week 8

## Question 1 - Obtaining the GLRT Statistic




# Week 9


# Week 10


# Week 11


# Week 12



# Week 13


