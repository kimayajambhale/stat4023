---
title: "Computer Labs"
# author: 'SID: 510472357'
date: "University of Sydney | Semester 2 `r format(Sys.time(), '%Y')`"
output: 
  html_document:
    code_folding: show  
    code_download: true 
    toc: true 
    toc_depth: 3
    toc_float: true 
    number_section: false 
    theme:
      bootswatch: "cosmo"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

* `pbinom`: cdf

* `dbinom`: pdf

* `qbinom`: quantile 

* `rbinom`: generate random values


# Week 2

## Question 1

**a)** Using `rbinom`, generate 1000 Binomial(200, 0.3) random variables and store them in `X`. Plot a histogram of `X` using `hist`, and overlay it with a normal density curve with appropriate mean and standard deviation using `curve` and `dnorm`.


```{r}
set.seed(4023)

X<-rbinom(1000,200,0.3)
hist(X, prob=TRUE, main="distribution of binomial(200,0.3)")
x_mean<-200*0.3
x_sd<-sqrt(200*0.3*0.7)
curve(dnorm(x,mean=x_mean, sd=x_sd), xlim=c(0,200), add=T)
```

**b)** Find $P(45 ≤ X ≤ 55)$ using either pbinom or dbinom.

```{r}
pbinom(55,200,0.3)-pbinom(44,200,0.3)
```


```{r}
dbinom(45:55,200,0.3)
sum(dbinom(45:55,200,0.3))
```

**c)** Using `pnorm`, compute the same probability with the normal approximation and normal
approximation with continuity correction. Comment on how close the approximations are.

Normal approximation:

```{r}
pnorm(55, mean=x_mean, sd=x_sd) - pnorm(45,mean=x_mean, sd=x_sd) 
```

Normal approximation with continuity correction gives a closer approximation.

```{r}
pnorm(55.5, mean=x_mean, sd=x_sd) - pnorm(44.5, mean=x_mean, sd=x_sd)
```

- the normal cdf cannot approximate binom cdf well at the jump points, bc its continuous whereas binom is discrete, but it approximates it very well half-way between these points of the binom cdf

## Question 2

**a)** Generate 1000 Binomial(200, 0.03) random variables (`rbinom`) and 1000 Poisson(6) random variables (`rpois`), store them in `X` and `Y` respectively. Set up a plotting environment with 1 row and 2 columns (`par(mfrow=c(1,2))`), and generate histograms of `X` and `Y`. Comment on the similarity of the two distributions.

```{r}
set.seed(3023)
par(mfrow=c(1,2))
X<-rbinom(1000,200,0.03)
hist(X, breaks=15, prob=TRUE, xlim=c(0,15), ylim=c(0,0.18), main="Binomial(200,0.03)")
Y<-rpois(1000,6)
hist(Y, breaks=15, prob=TRUE, xlim=c(0,15), ylim=c(0,0.18), main="Poisson(6)")
```

- as long as the mean of the binom is not too close to the end points (as long as theres no major skewness) normal is good approximator
- otherwise use poisson - better

**b)** Find $P (X ≤ 5)$ with `pbinom` and $P (Y ≤ 5)$ with `ppois`. Compare the two results.

```{r}
pbinom(5,200,0.03)
ppois(5,6)
```

- very close


**c)** Approximate $P (X ≤ 5)$ with the normal approximation (`pnorm`). How close is it this time?

```{r}
pnorm(5,mean=200*0.03,sd=sqrt(200*0.03*0.97))
```

- not accurate in this case 

## Question 3

Recall that for $n$ iid random variables $X_1,...,X_n,$ the standardised sum is given by

$$S = \frac{\sum_{i=1}^{n} (X_i - E(X_i))}{\sqrt{n\text{Var}(X_1)}}.$$

**a)** Generate 1000 realisations of the sum of $n$ $(n = 5)$ Unif(0,1) random variables (using `runif`), compute the standardised the sums and store them in `S1`. Repeat for $n = 100$ and store the results in `S2`.

Note that $E(X_i) = \frac{1}{2}$ and $\text{Var}(X_i) = \frac{1}{12}$

```{r}
set.seed(3023)

n <- 5
k <- 1000

result1 <- rep(0,k)

for (i in 1:k) {
  result1[i] <- sum(runif(n))
}

S1 <- (result1 - 0.5 * n) / sqrt(n * 1 / 12)



# repeating for n=100
n <- 100

result2 <- rep(0, k)
for (i in 1:k) {
  result2[i] <- sum(runif(n))
}

S2 <- (result2 - 0.5 * n) / sqrt(n * 1 / 12
)
```

**b)** Plot histograms of `S1` and `S2`, overlay them with the density curve of standard normal, and place these two plots side by side using `par(mfrow=c(1,2))`. Comment on how close the normal approximation is for the two values of $n$. 

```{r}
par(mfrow=c(1,2))
hist(S1, prob=TRUE, ylim=c(0,0.4), xlim=c(-3,3))
curve(dnorm(x,mean=0, sd=1),add=T)
hist(S2, prob=TRUE, ylim=c(0,0.4), xlim=c(-3,3))
curve(dnorm(x,mean=0, sd=1),add=T)
```

- both n give good approximations

**c)** Repeat (a) and (b) for the sum of lognormal(0,1) distributions (use `rlnorm(n, meanlog = 0, sdlog = 1)`). In this case, $E(X_i) = exp(1/2), Var(X_i) = (exp(1) − 1) exp(1).$

```{r}
set.seed(3023)


n <- 5
k <- 1000
result1 <- rep(0, k)

for (i in 1:k) {
  result1[i] <- sum(rlnorm(n, meanlog = 0, sdlog = 1))
}

S1 = (result1 - exp(1 / 2) * n) / sqrt(n * (exp(1) - 1) * exp(1))


n <- 100
k <- 1000
result2 <- rep(0, k)

for (i in 1:k) {
  result2[i] <- sum(rlnorm(n, meanlog = 0, sdlog = 1))
}

S2 <- (result2 - exp(1 / 2) * n) / sqrt(n * (exp(1) - 1) * exp(1))


par(mfrow = c(1, 2))
hist(S1, prob = TRUE)
curve(dnorm(x, mean = 0, sd = 1), add = T)
hist(S2, prob = TRUE)
curve(dnorm(x, mean = 0, sd = 1), add = T)
```


- In this case, the approximation gets better for larger $n$. Note that lognormal has a heavy tail distribution, so the convergence is slower.


**Q:** how many samples do we need before we can use normal dist to approximate a dist?

Ans: it depends on how "non-normal" the dist is. if perfectly normal, then even very small sample size is sufficient to use normal dist



# Week 3

## Question 1

**a)** Generate 100 realisations of the sample variance of 8 independent $N(0,1)$ random variables and store them in `s2`.

```{r}
set.seed(4023)

s2 <- vector()

for (i in 1:100) {
  s2[i] <- # storing the result as the ith element of s2
    var(rnorm(8, mean = 0, sd = 1)) # generating 8 iid N(0,1) RVs and calculating their variance
}
```

**b)** Plot the histogram of `(8-1)*s2` and overlay it with the density function of the $\chi^2_7$ distribution (use `dchisq`).

```{r}
hist((8-1)*s2, prob=TRUE)
curve(dchisq(x, df = 7), xlim=c(0,50), add=T)
```


**c)** Repeat(a) and (b) with $n = 50$ independent $N(0,1)$ random variables. Overlay the histogram with both the density curve of $\chi^2_{n−1}$ for an appropriate $\nu$ and the density curve of $N (n − 1, 2(n − 1))$ (using two different colours). Comment on the fit. Explain *briefly* why the normal approximation is suitable here.

```{r}
set.seed(4023)

s2 <- vector() # initialising vector 

for (i in 1:100) {
  s2[i] <- # storing the result as the ith element of s2
    var(rnorm(50, mean = 0, sd = 1)) # generating 50 iid N(0,1) RVs and calculating their variance
}

hist((49)*s2, prob=TRUE)
curve(dchisq(x, df = 49), xlim=c(0,100), add=T, col = "red", lty=1)
curve(dnorm(x, mean = 49, sd = (2*49)**0.5), add = T, col = "blue", lty=1)
legend("topright", legend = c(expression(chi[n-1]^2), expression(N(n-1,2(n-1)))),
       col = c("red", "blue"), lty=1:1, cex=0.8)
```

The density curve of $N (n − 1, 2(n − 1))$ closely follows the density curve of $\chi^2_{n−1}$, and fits the histogram as well as the density curve of $\chi^2_{n-1}$, hence using normal approximation is suitable here. Note that the $\chi^2$ density curve more closely follows the histogram here than in the previous question, since here we have a greater $n$. 

- when the df is large, we can apply the normal approximation due to the CLT

**d)** For $n = 50$, compute $P ((n − 1) \cdot S^2 > 54)$ using both the exact distribution ($\chi^2_\nu$) and the normal approximation. Compare the results.

```{r}
n = 50

# using exact distribution:
1 - pchisq(54, df = n-1) 

# using normal approximation:
1 - pnorm(54, mean = n-1, 
          sd = (2*(n-1))**0.5) # sqrt
```


Using the exact distribution and normal approximation yields very similar probabilities. The normal approximation gives a slightly higher value as its peak is slightly more to the right. 

- reasonably close


## Question 2

**a)** When two random variables $(X, Y )$ follow a bivariate normal distribution, the covariance matrix $\Sigma$ is defined as 

$$\Sigma = \begin{pmatrix}
\sigma_1^2 & \rho \sigma_1 \sigma_2 \\
\rho \sigma_1 \sigma_2 & \sigma_2^2
\end{pmatrix}, $$

where $\rho$ is the correlation, $\sigma_1^2, \sigma_2^2$ are the variances of $X$ and $Y$ respectively. Use `mvrnorm` from the MASS library (use `library(MASS)`) to generate 500 samples from a bivariate normal distribution with $\mu = (\mu_1,\mu_2)$ with $\mu_1 = 1, \mu_2 = 2,$ and 

$$\Sigma = \begin{pmatrix}
1 & 0.5 \\
0.5 & 4
\end{pmatrix}. $$

Store the first column as `x` and the second column as `y`.


```{r}
set.seed(4023)
library(MASS)

sigma <- matrix(c(1,0.5,0.5,4),2,2) # specifying the covariance matrix

q2a <- mvrnorm(n = 500, mu = c(1,2), Sigma = sigma)
x <- q2a[,1]
y <- q2a[,2]
```


**b)** Plot the histogram of `x` and overlay it with the corresponding marginal normal density. Repeat for `y`. (Recall the marginal distribution of $X$ is $N(\mu_1,\sigma_1^2)$.)

```{r}
hist(x, prob=TRUE)
curve(dnorm(x, mean = 1, sd = 1), xlim=c(-10,10), add=T)
```

```{r}
hist(y, prob=TRUE)
curve(dnorm(x, mean = 2, sd = 2), xlim=c(-10,10), add=T)
```


**c)** Produce a scatter plot of `x` and `y` (use `plot`). Compute the sample correlation coefficient (use `cor`) and comment on how close it is to the population correlation $\rho$. (First work out $\rho$ in the $\Sigma$ given.) 

```{r}
plot(x,y,
     xlab = "x", ylab = "y")

cor(x,y)
```

Theoretically, $\sigma_1 = 1$ and $\sigma_2 = 2 \Rightarrow$ population correlation coefficient $\rho = 0.25$, which is fairly close to the sample correlation coefficient, 0.296. 

## Question 3

**a)** Generate 100 realisations of the uniform U(0,1) distribution and store it in `u`. Apply the transformation $z = − log (u)$. Verify that $z$ is a sample from an exponential(1) distribution by plot the histogram of $z$ and overlay it with the density of the distribution.

```{r}
set.seed(3023)

u <- runif(n = 100)
z <- -1*log(u)

hist(z, freq = FALSE)
curve(dexp(x, rate = 1), add=T)
```

**b)** Generate 100 realisations of the minimum of 10 independent exponential(1) random variables. Note the rate parameter in `rexp` is defined as the reciprocal of the expectation (check the density function in the help file `?rexp`).

```{r}
set.seed(3023)

x_mat <- 
  rexp(n=100*10, rate = 1) |> matrix(nrow=10, ncol=100)

# we need to take the minimum of each of 10 trials
y_mat <- apply(x_mat, 
               2, # applying over columns (1 if row)
               min)
```

alternative:

```{r}
# x_mat <- rexp(n=100*10, rate = 1) |> matrix(ncol = 10, nrow = 100)
# y_mat <- apply(x_mat,
#                1,
#                min)
```


**c)** Plot the histogram and overlay it with the density of exponential(1/10) (`rate=10`) distribution. Comment on the fit.

```{r}
hist(y_mat, freq = FALSE, main = "minimum of iid exponential")
curve(dexp(x, rate=10), add = TRUE)
```


# Week 4

## Question 1

**a)** Generate 500 iid Poisson(5) (use `rpois`) random variables and store them in a vector `N`.

```{r}
N <- rpois(n=500, lambda = 5)
```


**b)** Now for each generated element `n` in `N`, generate 1 random variable following a binomial distribution with $n$ trials and success probabilities 0.20. Store all the generated values into a vector `X`.

```{r}
X <- rbinom(n=500,
       size = N, # size of trials (n)
       prob = 0.2)
```

**c)** Plot the histogram of `X`. What is the marginal distribution of $X$? Compare the relatively frequency for each value in $X$ versus the pmf of this distribution.

```{r}
hist(X)
```

- X has a Poisson(5*0.2) = Poisson(1) distribution 

Frequency of each value in X:

```{r}
table(X)/500
```

pmf of Poisson(1):

```{r}
dpois(min(X):max(X), lambda = 1) |> round(3)
```

- relative frequency of each outcome is very close to the pmf evaluated by the poisson distribution with rate $\lambda = 1$

## Question 2: Transformation of Random Variables

**a)** Generate 100 random variables from the standard normal distribution, and store them in a vector `z`. Apply the transformation `u <- z^2`. Overlay the histogram of `u` with the density curve of a $\chi^2_1$ distribution (use `dchisq(x, 1)`. Also verify that the $\chi^2_1$ distribution is the same as the Gamma distribution with rate 1/2 and scale 2 (use `dgamma(x, shape = 1/2, scale = 2)`), Comment on the plot.


```{r}
z <- rnorm(n=100)
u <- z^2

hist(u, freq = FALSE)
curve(dchisq(x, 1), add=TRUE, col="blue")
curve(dgamma(x, shape = 1/2, scale = 2), add=TRUE, col="red", lty=2)
```

- the 3 distributions agree

**b)** Generate 100 random variables from a $t$ distribution with 5 degrees of freedom (use `rt(100,df=5)`). Store them in $t$. Make another vector `f` by `f <- t^2`. Overlay the histogram of `f` with the density curve of a $F_{1,5}$ distribution (use `df(x, df1=1, df2=5)`). Comment on the plot. 

```{r}
t <- rt(100, df=5)
f <- t^2

hist(f, freq=FALSE, main="square of t distrib")
curve(df(x, df1=1, df2=5), add = TRUE)
```

- the 2 distributions agree

**c)** Generate 100 random variables from a $F_{5,2}$ distribution (use `rf(100, df1=5, df2=2)`). Store them in `y`. Make another vector `w <- 1/y`. Overlay the histogram of `w` with the density curve of a $F_{2,5}$ distribution. Comment on the plot.

```{r}
set.seed(3023)

y <- rf(100, df1=5, df2=2)
w <- 1/y

hist(w, freq=F, main="inverse of an F distrib", ylim=c(0,0.8))
curve(df(x, df1=2, df2=5), add = TRUE)
```

- the 2 distributions agree

**d)** Generate 100 random variables from a beta(2, 1) distribution (use `rbeta(100, shape1=2, shape2=1)`). Store them in `z`. Make another vector `v <- 2*z/(4*(1-z))`. Overlay the histogram of `v` with the density curve of a $F_{4,2}$ distribution. Comment on the plot.

```{r}
z <- rbeta(100, shape1 = 2, shape2 = 1)
v <- 2*z/(4*(1-z))

hist(v, freq=FALSE, ylim=c(0,0.5), main="transformation of beta distribution")
curve(df(x, df1=4, df2=2), add=TRUE)
```

- the 2 distributions agree


# Week 5

We shall compare three different estimators of a binomial success probability. If $Y ∼ Bin(2, θ)$ then we have

$$P(Y =0)=(1−θ)^2;$$

$$P(Y =1)=2θ(1−θ);$$

$$P(Y =2)=θ^2.$$

If we have an iid sample $Y_1,...,Y_n$ and define $N_0 = \sum_{i=1}^n 1 \{Yi = 0\}$ as the number of 0’s and $N_2 = \sum_{i=1}^n 1 \{Yi = 2\}$ as the number of 2’s, then we also have that

$$N_0 ∼ Bin(n,(1−θ)^2) \text{ and}$$

$$N_2 ∼ Bin(n,θ^2).$$


The usual estimator of θ based on an iid sample $Y_1,...,Y_n$ is a function of $\bar{Y} = \frac{1}{n} \sum_{i=1}^n Y_i$.

## Question 1

Determine an unbiased estimator for θ which is a *linear* function of $\bar{Y}$ Call it $\hat{\theta}_1$.

$$Y \sim Bin(2,\theta) \Rightarrow E(Y) = 2\theta$$

$$E(\bar{Y}) = \frac{1}{n} \sum_{i=1}^n E(Y_i) = \frac{1}{n} * n(2\theta) = 2\theta$$
$\Rightarrow \hat{\theta}_1 = \bar{Y}/2$ is an unbiased estimator of $\theta$

## Question 2

Determine an estimator for θ which is a *nonlinear* function of $N_0$ (hint: use method of moments, i.e. set $N_0$ equal to expectation and solve for θ). Call it $\hat{\theta}_0$. 

$E(N_0) = n(1-\theta)^2,$ setting $N_0 = n(1-\theta)^2$ gives $\hat{\theta}_0 = 1 = \sqrt{\frac{N_0}{n}}$

## Question 3

Determine an estimator for θ which is a *nonlinear* function of N2 (hint: use method of moments, i.e. set $N_2$ equal to expectation and solve for θ). Call it $\hat{\theta}_2$.

(similar)

## Question 4

We shall simulate a sample of $n = 100$ iid $Y_i$’s and compute the values of these three estimators, followed by comparing their mean squared errors, for a fine grid of θ values. 

(full question on sheet)

```{r}

```










# Week 6


# Week 7


# Week 8


# Week 9


# Week 10


# Week 11


# Week 12



# Week 13


